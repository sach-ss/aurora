# ğŸŒŒ Aurora  
**Conversational RAG-based Coding Assistant for Impact Analysis**

---

[![Python](https://img.shields.io/badge/Python-3.10+-blue?logo=python&logoColor=white)](https://www.python.org/)
[![Ollama](https://img.shields.io/badge/Ollama-Local_LLMs-black?logo=ollama)](https://ollama.com)
[![Gradio](https://img.shields.io/badge/Frontend-Gradio-orange?logo=gradio)](https://gradio.app)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Status](https://img.shields.io/badge/Build-Passing-success.svg)]()

---

Aurora is a **local conversational RAG assistant** designed to perform **impact analysis** on codebases.  
It uses **Ollama LLMs** (Gemma 2B + Nomic Embeddings) and **Gradio UI** to let you chat with your code locally â€” no cloud dependency.

---

## ğŸš€ Steps to Run Locally  

Follow these steps to get the application running on your PC.

---

### ğŸ§© **Step 1: Install Ollama**  
Go to [ollama.com](https://ollama.com) and download the application for Windows.  

Run the installer â€” Ollama will start automatically in the background.

---

### ğŸ¤– **Step 2: Download the Local Models**  
Open your terminal (Command Prompt or PowerShell).  

Run the following commands to pull the models (Gemma 2B for chat, Nomic for embeddings):

```bash
ollama pull gemma:2b
```

```bash
ollama pull nomic-embed-text
```

---

### ğŸ—‚ï¸ **Step 3: Set Up Your Python Project**  
Create your main project folder (e.g., `code-partner-rag-app`).  

Inside it, create the following folders:
- `rag_agent`
- `my_project_code`

Copy all the code files above into their correct locations.  

> âš ï¸ **Important:** Place the code files you want to analyze into the `my_project_code` folder.

---

### ğŸ§± **Step 4: Install Python Dependencies**  
Open your terminal in the project folder (`code-partner-rag-app`).  

Itâ€™s recommended to use a Python virtual environment:

```bash
python -m venv venv
source venv/Scripts/activate
```

Install all required dependencies:

```bash
pip install -r requirements.txt
```

---

### ğŸ§® **Step 5: Ingest Your Code (Create Vector Store)**  
Ensure Ollama is running in the background, then execute:

```bash
python data_ingestion.py
```

This reads your project files, generates embeddings, and stores them in the `vector_store` folder.  
Run this again only if you **add or modify** files in `my_project_code`.

---

### ğŸ’» **Step 6: Run the Gradio App!**  
Start the web interface with:

```bash
python app.py
```

Your terminal will show:  
- **Local URL:** `http://127.0.0.1:7860`  
- **Public URL:** `https://xxxxx.gradio.live`  

Open either in your browser to chat with your code! ğŸ§ 

---

## ğŸ§  Tech Stack  
- **Backend:** Python, LangChain, FAISS, Ollama  
- **Frontend:** Gradio  
- **Embeddings:** Nomic Embed  
- **LLM:** Gemma 2B  

---

## ğŸ“‚ Project Structure  

```
code-partner-rag-app/
â”œâ”€â”€ rag_agent/
â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ my_project_code/
â”‚   â””â”€â”€ your_source_files.py
â”œâ”€â”€ vector_store/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“œ License  
This project is licensed under the **MIT License** â€” see the [LICENSE](LICENSE) file for details.

---

## âœ¨ Acknowledgments  
- [Ollama](https://ollama.com) for local LLM support  
- [Nomic](https://nomic.ai) for open embeddings  
- [Gradio](https://gradio.app) for the intuitive web interface  

---

> ğŸ’¡ *â€œAurora turns your codebase into a conversational partner â€” analyze, query, and explore your projects with AI.â€*
